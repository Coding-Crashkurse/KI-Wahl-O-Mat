from langchain.chains import ConversationalRetrievalChain
from langchain.llms import OpenAI
import pickle
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from dotenv import load_dotenv, find_dotenv

load_dotenv(find_dotenv())

with open("vectorstore.pkl", "rb") as f:
    vectorstore = pickle.load(f)

memory = ConversationBufferMemory(
    memory_key="chat_history", return_messages=True, output_key="answer"
)

prompt_template = """Du bist ein hilfreicher Assistent, der User bei der Wahlentscheidung zur Bayernwahl hilft.
Erstelle aus der Frage und dem Context eine finale Antwort f√ºr den User.

Context: {context}

Frage: {question}
Antwort hier:"""
PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)

qa = ConversationalRetrievalChain.from_llm(
    llm=OpenAI(),
    memory=memory,
    retriever=vectorstore.as_retriever(),
    combine_docs_chain_kwargs={"prompt": PROMPT},
)

while True:
    user_input = input("You: ")

    if user_input.lower() in ["exit", "quit", "bye"]:
        print("Goodbye!")
        break

    response = qa({"question": user_input})

    print("Bot:", response["answer"])
